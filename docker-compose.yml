version: "3.9"

services:
  dev:
    build:
      context: .devcontainer
      dockerfile: Dockerfile
    volumes:
      - .:/workspace:cached
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /workspace
    tty: true
    stdin_open: true
    environment:
      - PIP_DISABLE_PIP_VERSION_CHECK=1
    command: bash -lc "sleep infinity"

  # -------- TSG Officer (LangGraph + Streamlit) --------
  tsg-officer:
    build:
      context: services/tsg-officer
      dockerfile: Dockerfile
    profiles: ["tsg"]
    ports:
      - "8501:8501"
    environment:
      - PYTHONIOENCODING=utf-8
      - TSG_LLM_PROVIDER=${TSG_LLM_PROVIDER:-mock}
      - TSG_OPENAI_MODEL=${TSG_OPENAI_MODEL:-gpt-4o-mini}
      - TSG_CHECKPOINT_DB=/data/.tsg_checkpoints.sqlite
      # Used only when TSG_LLM_PROVIDER=openai
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Optional (enterprise proxies / compatible endpoints)
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
    volumes:
      - ./services/tsg-officer:/app
      - ./data/tsg-officer:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 10s
      timeout: 3s
      retries: 10
    restart: unless-stopped

  # Base cache/queue that many PoCs want
  redis:
    image: redis:7
    ports:
      - "6379:6379"

  # -------- RAG profile --------
  qdrant:
    image: qdrant/qdrant:latest
    profiles: ["rag"]
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  ollama:
    image: ollama/ollama:latest
    profiles: ["rag"]
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 10

  api:
    build:
      context: services/api
      dockerfile: Dockerfile
    profiles: ["rag"]
    environment:
      # provider 切到 ollama（也保留 OPENAI 变量，随时可切回）
      - PROVIDER=${PROVIDER:-ollama}
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
      - CHAT_MODEL=${CHAT_MODEL:-llama3.2:3b}
      # 如需继续用 OpenAI，只需把 PROVIDER=openai 并提供 OPENAI_API_KEY
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
    depends_on:
      - qdrant
      - ollama
    ports:
      - "8000:8000"
    volumes:
      - ./data/docs:/data/docs:ro

  # -------- MCP profile --------
  mcp-gateway:
    build:
      context: services/mcp-gateway
      dockerfile: Dockerfile
    profiles: ["mcp"]
    ports:
      - "6000:6000"

volumes:
  qdrant_data:
  ollama:
